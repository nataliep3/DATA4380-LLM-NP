{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c78a550-df1a-4071-bb85-041b79b44d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessities\n",
    "import json\n",
    "from openai import OpenAI, OpenAIError\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "256344ae-80ed-47f8-ab58-2506afacf967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing abstract and summary for new18.pdf...\n",
      "Comparing abstract and summary for new24.pdf...\n",
      "Comparing abstract and summary for psychpaper14.pdf...\n",
      "Comparing abstract and summary for psychpaper15.pdf...\n",
      "Comparing abstract and summary for psychpaper01.pdf...\n",
      "Comparing abstract and summary for new25.pdf...\n",
      "Comparing abstract and summary for new19.pdf...\n",
      "Comparing abstract and summary for psychpaper17.pdf...\n",
      "Comparing abstract and summary for psychpaper03.pdf...\n",
      "Comparing abstract and summary for psychpaper02.pdf...\n",
      "Comparing abstract and summary for psychpaper16.pdf...\n",
      "Comparing abstract and summary for new22.pdf...\n",
      "Comparing abstract and summary for psychpaper12.pdf...\n",
      "Comparing abstract and summary for psychpaper07.pdf...\n",
      "Comparing abstract and summary for psychpaper13.pdf...\n",
      "Comparing abstract and summary for new23.pdf...\n",
      "Comparing abstract and summary for new21.pdf...\n",
      "Comparing abstract and summary for new09.pdf...\n",
      "Comparing abstract and summary for psychpaper05.pdf...\n",
      "Comparing abstract and summary for psychpaper11.pdf...\n",
      "Comparing abstract and summary for psychpaper10.pdf...\n",
      "Comparing abstract and summary for psychpaper04.pdf...\n",
      "Comparing abstract and summary for new08.pdf...\n",
      "Comparing abstract and summary for new20.pdf...\n",
      "Comparing abstract and summary for new05.pdf...\n",
      "Comparing abstract and summary for new11.pdf...\n",
      "Comparing abstract and summary for psychpaper21.pdf...\n",
      "Comparing abstract and summary for psychpaper09.pdf...\n",
      "Comparing abstract and summary for psychpaper08.pdf...\n",
      "Comparing abstract and summary for psychpaper20.pdf...\n",
      "Comparing abstract and summary for new10.pdf...\n",
      "Context length exceeded. Chunking summary...\n",
      "Comparing chunk 1 of 1...\n",
      "Error comparing new10.pdf: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 30314 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Comparing abstract and summary for new04.pdf...\n",
      "Comparing abstract and summary for new12.pdf...\n",
      "Comparing abstract and summary for new06.pdf...\n",
      "Comparing abstract and summary for psychpaper22.pdf...\n",
      "Comparing abstract and summary for psychpaper23.pdf...\n",
      "Comparing abstract and summary for new07.pdf...\n",
      "Comparing abstract and summary for new13.pdf...\n",
      "Comparing abstract and summary for new17.pdf...\n",
      "Comparing abstract and summary for new03.pdf...\n",
      "Comparing abstract and summary for new02.pdf...\n",
      "Comparing abstract and summary for new16.pdf...\n",
      "Comparing abstract and summary for new14.pdf...\n",
      "Comparing abstract and summary for psychpaper18.pdf...\n",
      "Comparing abstract and summary for psychpaper24.pdf...\n",
      "Comparing abstract and summary for psychpaper25.pdf...\n",
      "Comparing abstract and summary for psychpaper19.pdf...\n",
      "Comparing abstract and summary for new15.pdf...\n",
      "Comparing abstract and summary for new01.pdf...\n",
      "Comparison completed and results saved to 'new_data/comparison_results.json'.\n"
     ]
    }
   ],
   "source": [
    "# loading API key\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def chunk_text(text, max_words=1500):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def parse_response(response_text):\n",
    "    try:\n",
    "        return json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"rating\": None, \"explanation\": response_text}\n",
    "\n",
    "def aggregate_results(results):\n",
    "    ratings = [r.get(\"rating\") for r in results if r.get(\"rating\") is not None]\n",
    "    explanations = [r.get(\"explanation\", \"\") for r in results]\n",
    "    avg_rating = sum(ratings) / len(ratings) if ratings else None\n",
    "    combined_explanation = \"\\n---\\n\".join(explanations)\n",
    "    return {\"average_rating\": avg_rating, \"combined_explanation\": combined_explanation}\n",
    "\n",
    "def call_compare_api(abstract, summary_chunk):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in psychology research. Compare the following abstract and summary:\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Summary:\n",
    "{summary_chunk}\n",
    "\n",
    "Please respond ONLY with JSON in this format:\n",
    "{{\"rating\": <number from 1 to 5>, \"explanation\": \"<text>\"}}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=300,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def compare_abstract_summary(abstract, summary):\n",
    "    try:\n",
    "        response_text = call_compare_api(abstract, summary)\n",
    "        return parse_response(response_text)\n",
    "    except Exception as e:\n",
    "        if \"context_length_exceeded\" in str(e):\n",
    "            print(\"Context length exceeded. Chunking summary...\")\n",
    "            summary_chunks = chunk_text(summary, max_words=1500)\n",
    "            results = []\n",
    "            for i, chunk in enumerate(summary_chunks, 1):\n",
    "                print(f\"Comparing chunk {i} of {len(summary_chunks)}...\")\n",
    "                chunk_resp = call_compare_api(abstract, chunk)\n",
    "                parsed = parse_response(chunk_resp)\n",
    "                results.append(parsed)\n",
    "            return aggregate_results(results)\n",
    "        else:\n",
    "            print(f\"Error in compare_abstract_summary: {e}\")\n",
    "            return {\"rating\": None, \"explanation\": str(e)}\n",
    "\n",
    "# loading JSON file with abstracts and summaries\n",
    "INPUT_JSON_PATH = \"new_data/summaries.json\"\n",
    "OUTPUT_JSON_PATH = \"new_data/comparison_results.json\"\n",
    "\n",
    "try:\n",
    "    with open(INPUT_JSON_PATH, \"r\") as f:\n",
    "        papers = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Input file {INPUT_JSON_PATH} not found.\")\n",
    "    papers = []\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "try:\n",
    "    for paper in papers:\n",
    "        filename = paper.get(\"filename\", \"unknown\")\n",
    "        abstract = paper.get(\"abstract\", \"\")\n",
    "        summary = paper.get(\"summary\", \"\")\n",
    "        print(f\"Comparing abstract and summary for {filename}...\")\n",
    "        try:\n",
    "            result = compare_abstract_summary(abstract, summary)\n",
    "            comparison_results[filename] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error comparing {filename}: {e}\")\n",
    "\n",
    "finally:\n",
    "    # saves results in case of error along the way\n",
    "    with open(OUTPUT_JSON_PATH, \"w\") as f:\n",
    "        json.dump(comparison_results, f, indent=2)\n",
    "    print(f\"Comparison completed and results saved to '{OUTPUT_JSON_PATH}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-venv)",
   "language": "python",
   "name": "llm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

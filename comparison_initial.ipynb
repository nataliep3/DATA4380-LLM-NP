{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c78a550-df1a-4071-bb85-041b79b44d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessities\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256344ae-80ed-47f8-ab58-2506afacf967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing summary to abstract for psychpaper14.pdf...\n",
      "Comparing summary to abstract for psychpaper15.pdf...\n",
      "Comparing summary to abstract for psychpaper01.pdf...\n",
      "Comparing summary to abstract for psychpaper17.pdf...\n",
      "Comparing summary to abstract for psychpaper03.pdf...\n",
      "Comparing summary to abstract for psychpaper02.pdf...\n",
      "Comparing summary to abstract for psychpaper16.pdf...\n",
      "Comparing summary to abstract for psychpaper12.pdf...\n",
      "Comparing summary to abstract for psychpaper07.pdf...\n",
      "Comparing summary to abstract for psychpaper13.pdf...\n",
      "Comparing summary to abstract for psychpaper05.pdf...\n",
      "Comparing summary to abstract for psychpaper11.pdf...\n",
      "Comparing summary to abstract for psychpaper10.pdf...\n",
      "Comparing summary to abstract for psychpaper04.pdf...\n",
      "Comparing summary to abstract for psychpaper21.pdf...\n",
      "Comparing summary to abstract for psychpaper09.pdf...\n",
      "Comparing summary to abstract for psychpaper08.pdf...\n",
      "Comparing summary to abstract for psychpaper20.pdf...\n",
      "Comparing summary to abstract for psychpaper22.pdf...\n",
      "Comparing summary to abstract for psychpaper23.pdf...\n",
      "Comparing summary to abstract for psychpaper18.pdf...\n",
      "Comparing summary to abstract for psychpaper24.pdf...\n",
      "Comparing summary to abstract for psychpaper25.pdf...\n",
      "Comparing summary to abstract for psychpaper19.pdf...\n",
      "Comparison results saved to /Users/nataliepegues/data4380.np/llm_workspace/data/comparisons.json\n"
     ]
    }
   ],
   "source": [
    "# loading API / key\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# setting up path variables\n",
    "DATA_DIR = \"/Users/nataliepegues/data4380.np/llm_workspace/data\"\n",
    "SUMMARIES_JSON = f\"{DATA_DIR}/summaries.json\"\n",
    "COMPARISONS_JSON = f\"{DATA_DIR}/comparisons.json\"\n",
    "\n",
    "with open(SUMMARIES_JSON, \"r\") as f:\n",
    "    summaries = json.load(f)\n",
    "\n",
    "# function to compare abstracts / summaries\n",
    "def compare_abstract_summary(abstract, summary):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in psychology research. Compare the following abstract and summary:\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Please:\n",
    "- Rate how well the summary captures the abstract's key points on a scale from 1 (poor) to 5 (excellent).\n",
    "- Provide a brief explanation of your rating.\n",
    "\n",
    "Respond ONLY in JSON format like:\n",
    "{{\"rating\": <number>, \"explanation\": \"<text>\"}}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", # model choice, using same as summarization\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "comparisons = []\n",
    "\n",
    "for entry in summaries:\n",
    "    filename = entry[\"filename\"]\n",
    "    abstract = entry.get(\"abstract\", \"\")\n",
    "    summary = entry.get(\"summary\", \"\")\n",
    "\n",
    "    # adding text as a 'pulse check'\n",
    "    print(f\"Comparing summary to abstract for {filename}...\")\n",
    "    result_text = compare_abstract_summary(abstract, summary)\n",
    "\n",
    "    # trying to safely parse JSON\n",
    "    try:\n",
    "        import json\n",
    "        result_json = json.loads(result_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Failed to parse JSON response for {filename}. Raw response:\")\n",
    "        print(result_text)\n",
    "        result_json = {\"rating\": None, \"explanation\": result_text}\n",
    "\n",
    "    comparisons.append({\n",
    "        \"filename\": filename,\n",
    "        \"rating\": result_json.get(\"rating\"),\n",
    "        \"explanation\": result_json.get(\"explanation\"),\n",
    "    })\n",
    "\n",
    "# saving comparison\n",
    "with open(COMPARISONS_JSON, \"w\") as f:\n",
    "    json.dump(comparisons, f, indent=2)\n",
    "\n",
    "# completion message\n",
    "print(f\"Comparison results saved to {COMPARISONS_JSON}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-venv)",
   "language": "python",
   "name": "llm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
